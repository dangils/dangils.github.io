I"3<h1 id="객체-탐지-object-detection">객체 탐지 (Object Detection)</h1>

<ul>
  <li>한 이미지에서 객체와 그 경계 상자(bounding box)를 탐지</li>
  <li>객체 탐지 알고리즘은 일반적으로 이미지를 입력으로 받고, 경계 상자와 객체 클래스 리스트를 출력</li>
  <li>경계 상자에 대해 그에 대응하는 예측 클래스와 클래스의 신뢰도(confidence)를 출력</li>
</ul>

<h2 id="applications">Applications</h2>

<ul>
  <li>자율 주행 자동차에서 다른 자동차와 보행자를 찾을 때</li>
  <li>의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때</li>
  <li>제조업에서 조립 로봇이 제품을 조립하거나 수리할 때</li>
  <li>보안 산업에서 위협을 탐지하거나 사람 수를 셀 때</li>
</ul>

<h2 id="bounding-box">Bounding Box</h2>

<ul>
  <li>
    <p>이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형</p>

    <p><img src="https://miro.medium.com/max/850/1*KL6r494Eyfh3iYEXQA2tzg.png" /></p>

    <h2 id="iouintersection-over-union">IOU(Intersection Over Union)</h2>
  </li>
  <li>
    <p>실측값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표</p>

    <p><img src="https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png" width="300" /></p>
  </li>
  <li>
    <p>IOU가 높을수록 잘 예측한 모델 (1에 가까울 수록 높은 수치)</p>

    <p><img src="https://pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png" width="400" /></p>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p>예시</p>

    <p><img src="https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_stop_sign.jpg" /></p>

    <h2 id="nmsnon-maximum-suppression-비최댓값-억제">NMS(Non-Maximum Suppression, 비최댓값 억제)</h2>
  </li>
  <li>확률이 가장 높은 상자와 겹치는 상자들을 제거하는 과정</li>
  <li>최댓값을 갖지 않는 상자들을 제거</li>
  <li>과정
    <ol>
      <li>확률 기준으로 모든 상자를 정렬하고 먼저 가장 확률이 높은 상자를 취함</li>
      <li>각 상자에 대해 다른 모든 상자와의 IOU를 계산</li>
      <li>특정 임곗값을 넘는 상자는 제거</li>
    </ol>

    <p><img src="https://pyimagesearch.com/wp-content/uploads/2014/10/nms_fast_03.jpg" /></p>
  </li>
</ul>

<hr />

<blockquote>
  <p>모델 성능 평가</p>
</blockquote>

<p><strong>정밀도(Precision)와 재현율(Recall)</strong></p>

<ul>
  <li>
    <p>일반적으로 객체 탐지 모델 평가에 사용되지는 않지만, 다른 지표를 계산하는 기본 지표 역할을 함</p>

    <ul>
      <li>True Positives(<code class="language-plaintext highlighter-rouge">TP</code>): 예측이 동일 클래스의 실제 상자와 일치하는지 측정</li>
      <li>False Positives(<code class="language-plaintext highlighter-rouge">FP</code>): 예측이 실제 상자와 일치하지 않는지 측정</li>
      <li>
        <p>False Negatives(<code class="language-plaintext highlighter-rouge">FN</code>): 실제 분류값이 그와 일치하는 예측을 갖지 못하는지 측정</p>
      </li>
      <li>모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정(FP)이 많아져서 정밀도가 낮아짐</li>
      <li>모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주하면 거짓부정(FN)이 많아져서 재현율이 낮아짐</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>정밀도-재현율 곡선(Precision-Recall Curve)</p>
</blockquote>

<ul>
  <li>신뢰도 임계값마다 모델의 정밀도와 재현율을 시각화</li>
  <li>모든 bounding box와 함께 모델이 예측의 정확성을 얼마나 확실하는지 0 ~ 1사이의 숫자로 나타내는 신뢰도를 출력</li>
  <li>임계값 T에 따라 정밀도와 재현율이 달라짐
    <ul>
      <li>임계값 T 이하의 예측은 제거함</li>
      <li>T가 1에 가까우면 정밀도는 높지만 재현율은 낮음</li>
      <li>놓치는 객체가 많아져서 재현율이 낮아짐. 즉, 신뢰도가 높은 예측만 유지하기때문에 정밀도는 높아짐</li>
      <li>T가 0에 가까우면 정밀도는 낮지만 재현율은 높음</li>
      <li>대부분의 예측을 유지하기때문에 재현율은 높아지고, 거짓긍정(FP)이 많아져서 정밀도가 낮아짐</li>
    </ul>
  </li>
  <li>예를 들어, 모델이 보행자를 탐지하고 있으면 특별한 이유없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 함</li>
  <li>모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 함</li>
</ul>

<p><img src="https://www.researchgate.net/profile/Davide_Chicco/publication/321672019/figure/fig1/AS:614279602511886@1523467078452/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the.png" /></p>

<blockquote>
  <p>AP (Average Precision, 평균 정밀도) 와 mAP(mean Average Precision)</p>
</blockquote>

<ul>
  <li>곡선의 아래 영역에 해당</li>
  <li>항상 1x1 정사각형으로 구성되어 있음
즉, 항상 0 ~ 1 사이의 값을 가짐</li>
  <li>단일 클래스에 대한 모델 성능 정보를 제공</li>
  <li>전역 점수를 얻기위해서 mAP를 사용</li>
  <li>
    <p>예를 들어, 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고, 그 숫자들의 평균을 다시 구함</p>
  </li>
  <li>mAP 사용
    <ul>
      <li>최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes와 Common Objects in Context(COCO)에서 mAP가 사용됨</li>
      <li>
        <p>COCO 데이터셋이 더 많은 클래스를 포함하고 있기 때문에 보통 Pascal VOC보다 점수가 더 낮게 나옴</p>

        <p><img src="https://www.researchgate.net/profile/Bong_Nam_Kang/publication/328939155/figure/tbl2/AS:692891936649218@1542209719916/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev.png" /></p>
      </li>
    </ul>
  </li>
</ul>

<hr />

<blockquote>
  <p>객체 탐지 (Object Detection)의 역사</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/74512114/148298942-0e6f2450-82a9-4b09-a427-8122cc642c48.png" alt="image" /></p>

<ul>
  <li>RCNN (2013)
    <ul>
      <li>Rich feature hierarchies for accurate object detection and semantic segmentation (https://arxiv.org/abs/1311.2524)</li>
      <li>물체 검출에 사용된 기존 방식인 sliding window는 background를 검출하는 소요되는 시간이 많았는데, 이를 개선시킨 기법으로 Region Proposal 방식 제안</li>
      <li>매우 높은 Detection이 가능하지만, 복잡한 아키텍처 및 학습 프로세스로 인해 Detection 시간이 매우 오래 걸림</li>
    </ul>
  </li>
  <li>SPP Net (2014)
    <ul>
      <li>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (https://arxiv.org/abs/1406.4729)</li>
      <li>RCNN의 문제를 Selective search로 해결하려 했지만, bounding box의 크기가 제각각인 문제가 있어서 FC Input에 고정된 사이즈로 제공하기 위한 방법 제안</li>
      <li>SPP은 RCNN에서 conv layer와 fc layer사이에 위치하여 서로 다른 feature map에 투영된 이미지를 고정된 값으로 풀링</li>
      <li>SPP를 이용해 RCNN에 비해 실행시간을 매우 단축시킴</li>
    </ul>
  </li>
  <li>Fast RCNN (2015)
    <ul>
      <li>Fast R-CNN (https://arxiv.org/abs/1504.08083)</li>
      <li>SPP layer를 ROI pooling으로 바꿔서 7x7 layer 1개로 해결</li>
      <li>SVM을 softmax로 대체하여 Classification 과 Regression Loss를 함께 반영한 Multi task Loss 사용</li>
      <li>ROI Pooling을 이용해 SPP보다 간단하고, RCNN에 비해 수행시간을 많이 줄임</li>
    </ul>
  </li>
  <li>Fater RCNN(2015)
    <ul>
      <li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (https://arxiv.org/abs/1506.01497)</li>
      <li>RPN(Region proposal network) + Fast RCNN 방식</li>
      <li>Selective Search를 대체하기 위한 Region Proposal Network구현</li>
      <li>RPN도 학습시켜서 전체를 end-to-end로 학습 가능 (GPU사용 가능)</li>
      <li>Region Proposal를 위해 Object가 있는지 없는지의 후보 Box인 Anchor Box 개념 사용</li>
      <li>Anchor Box를 도입해 FastRCNN에 비해 정확도를 높이고 속도를 향상시킴</li>
    </ul>
  </li>
  <li>SSD (2015)
    <ul>
      <li>SSD: Single Shot MultiBox Detector (https://arxiv.org/abs/1512.02325)</li>
      <li>Faster-RCNN은 region proposal과 anchor box를 이용한 검출의 2단계를 걸치는 과정에서 시간이 필요해 real-time(20~30 fps)으로는 어려움</li>
      <li>SSD는 Feature map의 size를 조정하고, 동시에 앵커박스를 같이 적용함으로써 1 shot으로 물체 검출이 가능</li>
      <li>real-time으로 사용할 정도의 성능을 갖춤 (30~40 fps)</li>
      <li>작은 이미지의 경우에 잘 인식하지 못하는 경우가 생겨서 data augmentation을 통해 mAP를 63에서 74로 비약적으로 높임</li>
    </ul>
  </li>
  <li>RetinaNet (2017)
    <ul>
      <li>Focal Loss for Dense Object Detection (https://arxiv.org/abs/1708.02002)</li>
      <li>RetinaNet이전에는 1-shot detection과 2-shot detection의 차이가 극명하게 나뉘어 속도를 선택하면 정확도를 trade-off 할 수 밖에 없는 상황</li>
      <li>RetinaNet은 Focal Loss라는 개념의 도입과 FPN 덕분에 기존 모델들보다 정확도도 높고 속도도 여타 1-shot detector와 비견되는 모델</li>
      <li>Detection에선 검출하고 싶은 물체와 (foreground object) 검출할 필요가 없는 배경 물체들이 있는데 (background object) 배경 물체의 숫자가 매우 많을 경우 배경 Loss를 적게 하더라도 숫자에 압도되어 배경의 Loss의 총합을 학습해버림 (예를 들어, 숲을 배경으로 하는 사람을 검출해야하는데 배경의 나무가 100개나 되다보니 사람의 특징이 아닌 나무가 있는 배경을 학습해버림)</li>
      <li>Focal Loss는 이런 문제를 기존의 crossentropy 함수에서 (1-sig)을 제곱하여 background object의 loss를 현저히 줄여버리는 방법으로 loss를 변동시켜 해결</li>
      <li>Focal Loss를 통해 검출하고자 하는 물체와 관련이 없는 background object들은 학습에 영향을 주지 않게 되고, 학습의 다양성이 더 넓어짐 (작은 물체, 큰 물체에 구애받지 않고 검출할 수 있게됨)</li>
      <li>실제로 RetinaNet은 object proposal을 2000개나 실시하여 이를 확인</li>
    </ul>
  </li>
  <li>Mask R-CNN (2018)
    <ul>
      <li>Mask R-CNN (https://arxiv.org/pdf/1703.06870.pdf)</li>
    </ul>
  </li>
  <li>YOLO (2018)
    <ul>
      <li>YOLOv3: An Incremental Improvement (https://arxiv.org/abs/1804.02767)</li>
      <li>YOLO는 v1, v2, v3의 순서로 발전하였는데, v1은 정확도가 너무 낮은 문제가 있었고 이 문제는 v2까지 이어짐</li>
      <li>엔지니어링적으로 보완한 v3는 v2보다 살짝 속도는 떨어지더라도 정확도를 대폭 높인 모델</li>
      <li>RetinaNet과 마찬가지로 FPN을 도입해 정확도를 높임</li>
      <li>RetinaNet에 비하면 정확도는 4mAP정도 떨어지지만, 속도는 더 빠르다는 장점</li>
    </ul>
  </li>
  <li>RefineDet (2018)
    <ul>
      <li>Single-Shot Refinement Neural Network for Object Detection (https://arxiv.org/pdf/1711.06897.pdf)</li>
    </ul>
  </li>
  <li>M2Det (2019)
    <ul>
      <li>M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network (https://arxiv.org/pdf/1811.04533.pdf)</li>
    </ul>
  </li>
  <li>EfficientDet (2019)
    <ul>
      <li>EfficientDet: Scalable and Efficient Object Detection (https://arxiv.org/pdf/1911.09070v1.pdf)</li>
    </ul>
  </li>
  <li>YOLOv4 (2020)
    <ul>
      <li>YOLOv4: Optimal Speed and Accuracy of Object Detection (https://arxiv.org/pdf/2004.10934v1.pdf)</li>
      <li>YOLOv3에 비해 AP, FPS가 각각 10%, 12% 증가</li>
      <li>YOLOv3와 다른 개발자인 AlexeyBochkousky가 발표</li>
      <li>v3에서 다양한 딥러닝 기법(WRC, CSP …) 등을 사용해 성능을 향상시킴</li>
      <li>CSPNet 기반의 backbone(CSPDarkNet53)을 설계하여 사용</li>
    </ul>
  </li>
  <li>YOLOv5 (2020)
    <ul>
      <li>YOLOv4에 비해 낮은 용량과 빠른 속도 (성능은 비슷)</li>
      <li>YOLOv4와 같은 CSPNet 기반의 backbone을 설계하여 사용</li>
      <li>YOLOv3를 PyTorch로 implementation한 GlennJocher가 발표</li>
      <li>Darknet이 아닌 PyTorch 구현이기 때문에, 이전 버전들과 다르다고 할 수 있음</li>
    </ul>
  </li>
  <li>이후
    <ul>
      <li>수 많은 YOLO 버전들이 탄생</li>
      <li>Object Detection 분야의 논문들이 계속해서 나오고 있음</li>
    </ul>
  </li>
</ul>

:ET